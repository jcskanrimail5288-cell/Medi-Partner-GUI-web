# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã®ã‚³ãƒãƒ³ãƒ‰
# pip install streamlit python-dotenv google-generativeai requests

import streamlit as st
import requests
import os
import time
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai.types import generation_types
import re

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- å®šæ•°å®šç¾© ---

# è¨­è¨ˆæ„å›³: AIã«åŸºæœ¬çš„ãªå½¹å‰²ã‚’ä¼ãˆã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚
PERSONA_PROMPT = """
ã‚ãªãŸã¯ã€ŒMedi-Partnerã€ã¨ã„ã†åå‰ã®ã€çµŒé¨“è±Šå¯ŒãªåŒ»ç™‚äº‹å‹™ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
æ—¥åŒ»æ¨™æº–ãƒ¬ã‚»ãƒ—ãƒˆã‚½ãƒ•ãƒˆï¼ˆORCAï¼‰ã«é–¢ã™ã‚‹ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ã€æä¾›ã•ã‚ŒãŸã€Œé–¢é€£ãƒãƒ‹ãƒ¥ã‚¢ãƒ«é …ç›®ã€ã®æƒ…å ±ã‚’æœ€å„ªå…ˆã§å‚ç…§ã—ã€æ­£ç¢ºã‹ã¤ä¸å¯§ãªè¨€è‘‰ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
å›ç­”ã®æœ€å¾Œã«ã¯ã€å¿…ãšå‚ç…§ã—ãŸãƒšãƒ¼ã‚¸ã®URLã‚’ã€Œå‚è€ƒãƒªãƒ³ã‚¯ï¼šã€ã¨ã—ã¦è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
URLã®å½¢å¼ã¯ `https://orcamanual.orca.med.or.jp/gairai/chapter/.../` ã¾ãŸã¯ `https://orcamanual.orca.med.or.jp/nyuin/chapter/.../` ã¨ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
"""

# ORCAãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®ç›®æ¬¡ãƒ‡ãƒ¼ã‚¿
GAIRAI_TOC = {
    "1.1": "glclient2ã«ã¤ã„ã¦", "1.2": "ãƒã‚¹ã‚¿ãƒ¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼", "1.3": "æ¥­å‹™ãƒ¡ãƒ‹ãƒ¥ãƒ¼",
    "2.1": "å—ä»˜", "2.2": "ç™»éŒ²(æ‚£è€…ç™»éŒ²ã«ã¤ã„ã¦)", "2.3": "ç…§ä¼š", "2.4": "äºˆç´„", "2.5": "è¨ºç™‚è¡Œç‚º", "2.6": "è¨ºç™‚åŒºåˆ†åˆ¥ã®å…¥åŠ›æ–¹æ³•", "2.7": "ç—…å", "2.8": "åç´", "2.9": "ä¼šè¨ˆç…§ä¼š", "2.10": "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå°åˆ·",
    "3.1": "ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯", "3.2": "æ˜ç´°æ›¸", "3.3": "è«‹æ±‚ç®¡ç†", "3.4": "ç·æ‹¬è¡¨ãƒ»å…¬è²»è«‹æ±‚æ›¸", "3.5": "æ—¥æ¬¡çµ±è¨ˆ", "3.6": "æœˆæ¬¡çµ±è¨ˆ", "3.7": "çœåºå¯¾å¿œ", "3.8": "æœ¬é™¢åˆ†é™¢æ©Ÿèƒ½", "3.9": "æ²»é¨“", "3.10": "ãƒ¦ãƒ¼ã‚¶ç®¡ç†", "3.11": "å¥åº·ä¿é™ºçµ„åˆãƒ»å…±æ¸ˆçµ„åˆã¸ã®ç›´æ¥è«‹æ±‚", "3.12": "å…¬è²»è¨˜è¼‰é †è¨­å®š", "3.13": "åŠ´ç½ãƒ¬ã‚»ãƒ—ãƒˆé›»ç®—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦", "3.14": "EFãƒ•ã‚¡ã‚¤ãƒ«ãƒ»æ§˜å¼4",
    "4.1": "ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›", "4.2": "å¤–éƒ¨åª’ä½“", "4.3": "ãƒã‚¹ã‚¿æ›´æ–°",
    "5.1": "ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ãƒã‚¹ã‚¿", "5.2": "ç‚¹æ•°ãƒã‚¹ã‚¿", "5.3": "ãƒ¦ãƒ¼ã‚¶ãŒè‡ªç”±ã«ç™»éŒ²ã§ãã‚‹ãƒã‚¹ã‚¿ã«ã¤ã„ã¦", "5.4": "ãƒã‚§ãƒƒã‚¯ãƒã‚¹ã‚¿", "5.5": "ä¿é™ºç•ªå·ãƒã‚¹ã‚¿", "5.6": "ä¿é™ºè€…ãƒã‚¹ã‚¿", "5.7": "äººåè¾æ›¸ãƒã‚¹ã‚¿", "5.8": "è–¬å‰¤æƒ…å ±ãƒã‚¹ã‚¿", "5.9": "ä½æ‰€ãƒã‚¹ã‚¿", "5.10": "ãƒ˜ãƒ«ãƒ—ãƒã‚¹ã‚¿",
    "6.1": "ä»˜éŒ²1", "6.2": "ä»˜éŒ²2", "6.3": "ä»˜éŒ²3", "6.4": "ä»˜éŒ²4", "6.5": "ä»˜éŒ²5",
    "7.1": "å¯¾å‡¦äº‹ä¾‹1", "7.2": "å¯¾å‡¦äº‹ä¾‹2", "7.3": "å¯¾å‡¦äº‹ä¾‹3", "7.4": "æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹æ„ŸæŸ“ç—‡ã«ä¿‚ã‚‹PCRæ¤œæŸ»"
}

NYUIN_TOC = {
    "1.1": "å…¥é™¢æ¥­å‹™ãƒ¡ãƒ‹ãƒ¥ãƒ¼", "1.2": "ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†æƒ…å ±ã®ç™»éŒ²ã«ã¤ã„ã¦", "1.3": "ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†æƒ…å ±ã®ç™»éŒ²",
    "2.1": "å…¥é€€é™¢ç™»éŒ²", "2.2": "å…¥é™¢ä¼šè¨ˆç…§ä¼šã«ã¤ã„ã¦", "2.3": "å…¥é™¢è¨ºç™‚è¡Œç‚ºå…¥åŠ›", "2.4": "åç´ç”»é¢ã‹ã‚‰ã®è«‹æ±‚å–æ¶ˆã—ã«ã¤ã„ã¦", "2.5": "é¸å®šå…¥é™¢æ–™ã«ã¤ã„ã¦", "2.6": "90æ—¥ã‚’è¶…ãˆã‚‹æ‚£è€…ã®å…¥é™¢æ–™ã«ã¤ã„ã¦", "2.7": "å…¥é™¢è¨ºç™‚è¡Œç‚ºç”»é¢ã‹ã‚‰ã®å…¥é™¢å‡¦æ–¹ã›ã‚“å°åˆ·ã«ã¤ã„ã¦", "2.8": "å…¥é™¢è¨ºç™‚è¡Œç‚ºç”»é¢ã‹ã‚‰ã®ãŠè–¬æ‰‹å¸³ç­‰å°åˆ·ã«ã¤ã„ã¦", "2.9": "æ¨™æ¬ ã«ã‚ˆã‚‹æ¸›é¡", "2.10": "å®šæ•°è¶…éå…¥é™¢", "2.11": "çŸ­æœŸæ»åœ¨æ‰‹è¡“ç­‰åŸºæœ¬æ–™3ã«ã¤ã„ã¦", "2.12": "æ€¥æ€§å¢—æ‚ªã«ã‚ˆã‚‹ä»‹è­·ç—…æ£Ÿã‹ã‚‰ã®ç•°å‹•ã«ã¤ã„ã¦", "2.13": "ä¸€èˆ¬ãƒ»ç™‚é¤Šç›¸äº’ç®—å®šã«ã¤ã„ã¦",
    "3.1": "å…¥é™¢å®šæœŸè«‹æ±‚", "3.2": "å…¥é™¢ä¼šè¨ˆä¸€æ‹¬ä½œæˆã«ã¤ã„ã¦",
    "4.1": "é€€é™¢æ™‚ä»®è¨ˆç®—ã«ã¤ã„ã¦", "4.2": "æ‚£è€…ç…§ä¼šã«ã¤ã„ã¦",
    "5.1": "ãƒ¬ã‚»ãƒ—ãƒˆä½œæˆã«ã¤ã„ã¦", "5.2": "å…¥é™¢ãƒ¬ã‚»ãƒ—ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆè‡ªå‹•è¨˜è¼‰ã«ã¤ã„ã¦", "5.3": "ç¦å²¡çœŒã®å…¥é™¢ãƒ¬ã‚»ãƒ—ãƒˆå¯¾å¿œã«ã¤ã„ã¦",
    "6.1": "æ’ä»–åˆ¶å¾¡",
    "7.1": "å…¥é™¢ç™»éŒ²æ™‚ã®è¨‚æ­£æ–¹æ³•ç­‰ã«ã¤ã„ã¦", "7.2": "å‡ºç”£è‚²å…ä¸€æ™‚é‡‘ç­‰ã®åŒ»ç™‚æ©Ÿé–¢ã¸ã®ç›´æ¥æ”¯æ‰•åˆ¶åº¦", "7.3": "å…¥é™¢æœŸé–“ä¸­ã®å¤–æ¥å…¥åŠ›", "7.4": "åŒ»ç™‚è¦³å¯Ÿæ³•", "7.5": "å…¥é™¢ã‚ªãƒ¼ãƒ€ãƒ¼", "7.6": "å›å¾©æœŸãƒªãƒãƒ“ãƒªãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç—…æ£Ÿå…¥é™¢æ–™ã®ç–¾æ‚£åˆ¥ãƒªãƒãƒ“ãƒªãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ–™åŒ…æ‹¬å…¥åŠ›", "7.7": "æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹æ„ŸæŸ“ç—‡å…¥é™¢å¯¾å¿œ",
    "8.1": "æ—¥æ¬¡çµ±è¨ˆå¸³ç¥¨ã«ã¤ã„ã¦",
    "9.1": "æœˆæ¬¡çµ±è¨ˆå¸³ç¥¨ã«ã¤ã„ã¦",
    "10.1": "å…¥é™¢å®¤æ–™åŠ ç®—ã®è¨­å®š", "10.2": "å…¥é™¢é£Ÿäº‹ç™‚é¤Šè²»ã®è¨­å®šï¼ˆè‡ªè³ è²¬ã®ã¿ï¼‰"
}

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---

def find_relevant_sections(query: str, toc: dict) -> list[str]:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨ç›®æ¬¡ã‹ã‚‰ã€é–¢é€£æ€§ã®é«˜ã„é …ç›®ã‚’ã„ãã¤ã‹æŠ½å‡ºã™ã‚‹ã€‚
    """
    scores = []
    # ã‚¯ã‚¨ãƒªã‹ã‚‰åè©ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ€ã‚ã‚Œã‚‹å˜èªã‚’æŠ½å‡ºï¼ˆç°¡å˜ãªæ­£è¦è¡¨ç¾ï¼‰
    query_words = set(re.findall(r'[ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9]+', query))
    
    for key, section in toc.items():
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã‹ã‚‰ã‚‚å˜èªã‚’æŠ½å‡º
        section_words = set(re.findall(r'[ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9]+', section))
        score = len(query_words.intersection(section_words))
        if score > 0:
            scores.append((score, f"{key} {section}"))
    
    scores.sort(key=lambda x: x[0], reverse=True)
    return [section for score, section in scores[:3]]

# --- AIå¿œç­”ç”Ÿæˆé–¢æ•° ---

def get_text_response_gemini(prompt: str) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ç›®æ¬¡æ¤œç´¢ã¨ã€Gemini APIå‘¼ã³å‡ºã—ã‚’çµ„ã¿åˆã‚ã›ãŸ2æ®µéšå‡¦ç†ã§å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    # --- ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ç›®æ¬¡æ¤œç´¢ ---
    if any(keyword in prompt for keyword in ["å…¥é™¢", "é€€é™¢", "ç—…æ£Ÿ", "å…¥é™¢æ–™"]):
        toc = NYUIN_TOC
        manual_type = "nyuin"
        manual_name = "å…¥é™¢ç‰ˆ"
    else:
        toc = GAIRAI_TOC
        manual_type = "gairai"
        manual_name = "å¤–æ¥ç‰ˆ"
    
    relevant_sections = find_relevant_sections(prompt, toc)
    
    if not relevant_sections:
        return "é–¢é€£ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚"

    # --- ã‚¹ãƒ†ãƒƒãƒ—2: AIã¸ã®çš„ã‚’çµã£ãŸè³ªå• ---
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return "ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    genai.configure(api_key=api_key)
    
    context_for_ai = f"""
ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç¨®åˆ¥: {manual_name}
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: ã€Œ{prompt}ã€
é–¢é€£ãƒãƒ‹ãƒ¥ã‚¢ãƒ«é …ç›®:
- {"\n- ".join(relevant_sections)}

ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
    full_prompt = f"{PERSONA_PROMPT}\n\n{context_for_ai}"

    # --- APIå‘¼ã³å‡ºã— ---
    try:
        # ã¾ãšã¯é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã§è©¦è¡Œ
        model = genai.GenerativeModel('gemini-1.5-pro-latest')
        response = model.generate_content(full_prompt)
        if not response.parts:
            raise Exception("APIã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return response.text
    except Exception as e:
        # å¤±æ•—ã—ãŸå ´åˆã€ã‚ˆã‚Šè»½é‡ã§é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã§å†è©¦è¡Œ
        try:
            model = genai.GenerativeModel('gemini-1.5-flash-latest')
            response = model.generate_content(full_prompt)
            if not response.parts:
                 raise Exception("è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ã‚‚APIã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return response.text
        except Exception as final_e:
            return f"ã‚¨ãƒ©ãƒ¼: APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {final_e}"

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†é–¢æ•° ---

def handle_prompt(prompt: str):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã—ã€AIã®å¿œç­”ã‚’ç”Ÿæˆãƒ»è¡¨ç¤ºã™ã‚‹
    """
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("AIãŒå¿œç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
            response = get_text_response_gemini(prompt)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

# --- Streamlit UIè¨­å®š ---

st.set_page_config(page_title="Medi-Partner", layout="wide")
st.title("ğŸ¥ Medi-Partner: ORCAå®Ÿå‹™è€…å‘ã‘AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼Medi-Partnerã§ã™ã€‚ORCAã‚„åŒ»ç™‚äº‹å‹™ã«é–¢ã™ã‚‹ã”è³ªå•ã‚’ã©ã†ãã€‚"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› ---
if prompt := st.chat_input("ORCAã«é–¢ã™ã‚‹è³ªå•ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§å…¥åŠ›ã—ã¦ãã ã•ã„"):
    handle_prompt(prompt)
